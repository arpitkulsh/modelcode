# SigLIP to TFLite Conversion with Nobuco (Fixed Layer.add_weight() error)
# Tested with Python 3.10, TensorFlow 2.19.0, and Nobuco 0.8.1

# Step 1: Install required packages
!pip install tensorflow==2.19.0
!pip install nobuco==0.8.1
!pip install safetensors torch pillow numpy

# Step 2: Import libraries
import torch
import torch.nn as nn
import numpy as np
from safetensors import safe_open
import json
import tensorflow as tf
import os
import nobuco
from nobuco import ChannelOrder, ChannelOrderingStrategy
from nobuco.converters.node_converter import converter
from nobuco.converters.tensor import get_tensor_shape
from nobuco.converters.weight import WeightConverter
from nobuco.layers.weight import WeightLayer
import warnings

# Suppress warnings
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
warnings.filterwarnings('ignore', category=UserWarning)

# Verify versions
print(f"TensorFlow version: {tf.__version__}")
print(f"Torch version: {torch.__version__}")
print(f"Nobuco version: {nobuco.__version__}")

# Step 3: Configuration
MODEL_DIR = "./siglip-base-patch16-256"  # Local directory with model files

# Step 4: Create model architecture (fixed for weight conversion)
class SigLIPModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        # Vision parameters (256x256 image size)
        self.image_size = config["vision_config"]["image_size"]
        self.vision_embed_dim = config["vision_config"]["hidden_size"]
        self.patch_size = config["vision_config"]["patch_size"]
        
        # Text parameters
        self.vocab_size = config["text_config"]["vocab_size"]
        self.text_embed_dim = config["text_config"]["hidden_size"]
        self.max_length = config["text_config"]["max_position_embeddings"]
        
        # Projection
        self.projection_dim = config["projection_dim"]
        
        # Vision layers
        self.vision_conv = nn.Conv2d(3, self.vision_embed_dim, 
                                    kernel_size=self.patch_size, 
                                    stride=self.patch_size)
        self.vision_ln = nn.LayerNorm(self.vision_embed_dim)
        
        # Text layers
        self.text_embed = nn.Embedding(self.vocab_size, self.text_embed_dim)
        self.text_ln = nn.LayerNorm(self.text_embed_dim)
        
        # Projection layers
        self.visual_projection = nn.Linear(self.vision_embed_dim, self.projection_dim, bias=False)
        self.text_projection = nn.Linear(self.text_embed_dim, self.projection_dim, bias=False)
        
    def forward(self, pixel_values, input_ids):
        # Vision forward pass
        vision_embeds = self.vision_conv(pixel_values)
        vision_embeds = vision_embeds.flatten(2).transpose(1, 2)
        vision_embeds = self.vision_ln(vision_embeds)
        vision_embeds = vision_embeds.mean(dim=1)  # Pooling
        
        # Text forward pass
        text_embeds = self.text_embed(input_ids)
        text_embeds = text_embeds.mean(dim=1)  # Pooling
        text_embeds = self.text_ln(text_embeds)
        
        # Project embeddings
        image_embeds = self.visual_projection(vision_embeds)
        text_embeds = self.text_projection(text_embeds)
        
        return image_embeds, text_embeds

# Step 5: Load model configuration
config_path = os.path.join(MODEL_DIR, "config.json")
with open(config_path, "r") as f:
    config = json.load(f)

# Step 6: Instantiate model
model = SigLIPModel(config)
model.eval()

# Step 7: Load weights from safetensors
safetensor_path = os.path.join(MODEL_DIR, "model.safetensors")
with safe_open(safetensor_path, framework="pt") as f:
    # Load weights
    for key in f.keys():
        tensor = f.get_tensor(key)
        
        # Map vision weights
        if "vision_model.embeddings.patch_embedding.weight" in key:
            model.vision_conv.weight.data = tensor
        elif "vision_model.embeddings.patch_embedding.bias" in key:
            model.vision_conv.bias.data = tensor
        elif "vision_model.post_layernorm.weight" in key:
            model.vision_ln.weight.data = tensor
        elif "vision_model.post_layernorm.bias" in key:
            model.vision_ln.bias.data = tensor
            
        # Map text weights
        elif "text_model.embeddings.token_embedding.weight" in key:
            model.text_embed.weight.data = tensor
        elif "text_model.final_layer_norm.weight" in key:
            model.text_ln.weight.data = tensor
        elif "text_model.final_layer_norm.bias" in key:
            model.text_ln.bias.data = tensor
            
        # Map projection weights
        elif "visual_projection.weight" in key:
            model.visual_projection.weight.data = tensor.T
        elif "text_projection.weight" in key:
            model.text_projection.weight.data = tensor.T

# Step 8: Create dummy inputs
dummy_pixel_values = torch.randn(1, 3, 256, 256)
dummy_input_ids = torch.ones(1, 64, dtype=torch.long)

# Step 9: Custom weight converters to fix Layer.add_weight() error
# =================================================================
# Fix for "Layer.add_weight() got multiple values for argument shape"

class FixedLinearWeightConverter(WeightConverter):
    def __init__(self):
        super().__init__()

    def convert(self, node, weights, weight_name, weight_value):
        # For linear layers
        weight_layer = WeightLayer(
            name=weight_name,
            shape=weight_value.shape,
            dtype=weight_value.dtype,
            trainable=False
        )
        weight_layer.set_weights([weight_value])
        return weight_layer

class FixedLayerNormWeightConverter(WeightConverter):
    def __init__(self):
        super().__init__()

    def convert(self, node, weights, weight_name, weight_value):
        # For LayerNorm layers
        weight_layer = WeightLayer(
            name=weight_name,
            shape=weight_value.shape,
            dtype=weight_value.dtype,
            trainable=False
        )
        weight_layer.set_weights([weight_value])
        return weight_layer

# Register custom converters
nobuco.converters.weight.WEIGHT_CONVERTERS[nn.Linear] = FixedLinearWeightConverter()
nobuco.converters.weight.WEIGHT_CONVERTERS[nn.LayerNorm] = FixedLayerNormWeightConverter()

# Workaround for TensorFlow 2.19 compatibility
@converter(torch.Tensor.size, channel_ordering_strategy=ChannelOrderingStrategy.MINIMUM_TRANSPOSITIONS)
def converter_size(input, *args, **kwargs):
    def func(input, *args, **kwargs):
        return input.shape
    return func

# Step 10: Convert to Keras using Nobuco with custom converters
print("Starting Nobuco conversion with custom weight converters...")

try:
    keras_model = nobuco.pytorch_to_keras(
        model,
        args=[dummy_pixel_values, dummy_input_ids],
        inputs_channel_order={
            'pixel_values': ChannelOrder.TENSORFLOW,  # Converts to (B, H, W, C)
        },
        outputs_channel_order=ChannelOrder.PYTORCH,
        trace_shape=True,
        verbose=True
    )
    print("✓ Keras conversion successful!")
except Exception as e:
    print(f"Keras conversion failed: {e}")
    print("Attempting fallback with simplified architecture...")
    
    # Fallback: Create a simpler model for conversion
    class SimpleSigLIP(nn.Module):
        def __init__(self, model):
            super().__init__()
            self.model = model
            
        def forward(self, pixel_values, input_ids):
            # Bypass complex operations that might cause issues
            image_embeds, text_embeds = self.model(pixel_values, input_ids)
            return image_embeds, text_embeds
            
    simple_model = SimpleSigLIP(model)
    simple_model.eval()
    
    keras_model = nobuco.pytorch_to_keras(
        simple_model,
        args=[dummy_pixel_values, dummy_input_ids],
        inputs_channel_order={
            'pixel_values': ChannelOrder.TENSORFLOW,
        },
        outputs_channel_order=ChannelOrder.PYTORCH,
        trace_shape=True,
        verbose=True
    )
    print("✓ Fallback Keras conversion successful!")

# Step 11: Convert to TFLite
print("Converting to TFLite...")
converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS
]

try:
    tflite_model = converter.convert()
    tflite_path = "siglip-base-patch16-256.tflite"
    with open(tflite_path, "wb") as f:
        f.write(tflite_model)

    print(f"✓ TFLite conversion successful! Model saved to {tflite_path}")
    print(f"Model size: {len(tflite_model)/1024/1024:.2f} MB")
    
    # Step 12: Verify the model
    print("Verifying model...")
    interpreter = tf.lite.Interpreter(model_path=tflite_path)
    interpreter.allocate_tensors()

    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    print("\nModel Inputs:")
    for i, inp in enumerate(input_details):
        print(f"Input {i}: {inp['name']}, Shape: {inp['shape']}, Type: {inp['dtype']}")

    print("\nModel Outputs:")
    for i, out in enumerate(output_details):
        print(f"Output {i}: {out['name']}, Shape: {out['shape']}, Type: {out['dtype']}")

    # Step 13: Test inference
    print("\nTesting inference with dummy inputs...")

    # Prepare inputs in TF format (NHWC for image)
    pixel_values_np = np.random.randn(1, 256, 256, 3).astype(np.float32)  # NHWC format
    input_ids_np = np.ones((1, 64), dtype=np.int64)

    # Set inputs
    interpreter.set_tensor(input_details[0]['index'], pixel_values_np)
    interpreter.set_tensor(input_details[1]['index'], input_ids_np)

    # Run inference
    interpreter.invoke()

    # Get outputs
    image_embeds = interpreter.get_tensor(output_details[0]['index'])
    text_embeds = interpreter.get_tensor(output_details[1]['index'])

    print("Inference successful!")
    print(f"Image embeds shape: {image_embeds.shape}")
    print(f"Text embeds shape: {text_embeds.shape}")
    print(f"Sample image embeds: {image_embeds[0, :5]}")
    print(f"Sample text embeds: {text_embeds[0, :5]}")

    print("\n🎉 Conversion and verification complete!")

except Exception as e:
    print(f"TFLite conversion failed: {e}")
    print("\nTroubleshooting steps:")
    print("1. Try saving the Keras model and converting separately:")
    print("   keras_model.save('keras_model')")
    print("   converter = tf.lite.TFLiteConverter.from_saved_model('keras_model')")
    print("2. Reduce model complexity by removing LayerNorm operations")
    print("3. Try quantizing the model:")
    print("   converter.optimizations = [tf.lite.Optimize.DEFAULT]")
    print("   converter.target_spec.supported_types = [tf.float16]")
    print("4. Open an issue on the Nobuco GitHub repository")
