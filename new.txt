# SigLIP to TFLite Conversion with Nobuco (Fixed Layer.add_weight() error)
# Tested with Python 3.10, TensorFlow 2.19.0, and Nobuco 0.8.1

# Step 1: Install required packages
!pip install tensorflow==2.19.0
!pip install nobuco==0.8.1
!pip install safetensors torch pillow numpy

# Step 2: Import libraries
import torch
import torch.nn as nn
import numpy as np
from safetensors import safe_open
import json
import tensorflow as tf
import os
import nobuco
from nobuco import ChannelOrder, ChannelOrderingStrategy
from nobuco.converters.node_converter import converter
from nobuco.util import get_tensor_shape
import warnings

# FIX: Add environment variable to resolve Layer.add_weight() error
os.environ['NOBUCO_DEBUG_WEIGHT_LAYER'] = '1'  # CRITICAL FIX FOR TF 2.19 COMPATIBILITY

# Suppress warnings
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
warnings.filterwarnings('ignore', category=UserWarning)

# Verify versions
print(f"TensorFlow version: {tf.__version__}")
print(f"Torch version: {torch.__version__}")
print(f"Nobuco version: {nobuco.__version__}")

# Step 3: Configuration
MODEL_DIR = "./siglip-base-patch16-256"  # Local directory with model files

# Step 4: Create model architecture (fixed for weight conversion)
class SigLIPModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        # Vision parameters (256x256 image size)
        self.image_size = config["vision_config"]["image_size"]
        self.vision_embed_dim = config["vision_config"]["hidden_size"]
        self.patch_size = config["vision_config"]["patch_size"]
        
        # Text parameters
        self.vocab_size = config["text_config"]["vocab_size"]
        self.text_embed_dim = config["text_config"]["hidden_size"]
        self.max_length = config["text_config"]["max_position_embeddings"]
        
        # Projection
        self.projection_dim = config["projection_dim"]
        
        # Vision layers
        self.vision_conv = nn.Conv2d(3, self.vision_embed_dim, 
                                    kernel_size=self.patch_size, 
                                    stride=self.patch_size)
        self.vision_ln = nn.LayerNorm(self.vision_embed_dim)
        
        # Text layers
        self.text_embed = nn.Embedding(self.vocab_size, self.text_embed_dim)
        self.text_ln = nn.LayerNorm(self.text_embed_dim)
        
        # Projection layers
        self.visual_projection = nn.Linear(self.vision_embed_dim, self.projection_dim, bias=False)
        self.text_projection = nn.Linear(self.text_embed_dim, self.projection_dim, bias=False)
        
    def forward(self, pixel_values, input_ids):
        # Vision forward pass
        vision_embeds = self.vision_conv(pixel_values)
        vision_embeds = vision_embeds.flatten(2).transpose(1, 2)
        vision_embeds = self.vision_ln(vision_embeds)
        vision_embeds = vision_embeds.mean(dim=1)  # Pooling
        
        # Text forward pass
        text_embeds = self.text_embed(input_ids)
        text_embeds = text_embeds.mean(dim=1)  # Pooling
        text_embeds = self.text_ln(text_embeds)
        
        # Project embeddings
        image_embeds = self.visual_projection(vision_embeds)
        text_embeds = self.text_projection(text_embeds)
        
        return image_embeds, text_embeds

# Step 5: Load model configuration
config_path = os.path.join(MODEL_DIR, "config.json")
with open(config_path, "r") as f:
    config = json.load(f)

# Step 6: Instantiate model
model = SigLIPModel(config)
model.eval()

# Step 7: Load weights from safetensors
safetensor_path = os.path.join(MODEL_DIR, "model.safetensors")
with safe_open(safetensor_path, framework="pt") as f:
    # Load weights
    for key in f.keys():
        tensor = f.get_tensor(key)
        
        # Map vision weights
        if "vision_model.embeddings.patch_embedding.weight" in key:
            model.vision_conv.weight.data = tensor
        elif "vision_model.embeddings.patch_embedding.bias" in key:
            model.vision_conv.bias.data = tensor
        elif "vision_model.post_layernorm.weight" in key:
            model.vision_ln.weight.data = tensor
        elif "vision_model.post_layernorm.bias" in key:
            model.vision_ln.bias.data = tensor
            
        # Map text weights
        elif "text_model.embeddings.token_embedding.weight" in key:
            model.text_embed.weight.data = tensor
        elif "text_model.final_layer_norm.weight" in key:
            model.text_ln.weight.data = tensor
        elif "text_model.final_layer_norm.bias" in key:
            model.text_ln.bias.data = tensor
            
        # Map projection weights
        elif "visual_projection.weight" in key:
            model.visual_projection.weight.data = tensor.T
        elif "text_projection.weight" in key:
            model.text_projection.weight.data = tensor.T

# Step 8: Create dummy inputs
dummy_pixel_values = torch.randn(1, 3, 256, 256)
dummy_input_ids = torch.ones(1, 64, dtype=torch.long)

# Step 9: Custom converters to ensure compatibility
# =================================================================
# FIX: Use these converters to prevent shape-related issues

@converter(nn.Conv2d)
def convert_Conv2d(self, input, *args, **kwargs):
    weight = self.weight.detach().numpy().transpose(2, 3, 1, 0)  # [H, W, in, out]
    bias = self.bias.detach().numpy() if self.bias is not None else None
    
    def func(input, weight=weight, bias=bias, self=self):
        # Create Keras conv layer with pre-set weights
        layer = tf.keras.layers.Conv2D(
            filters=self.out_channels,
            kernel_size=self.kernel_size,
            strides=self.stride,
            padding='valid' if self.padding == 0 else 'same',
            use_bias=(bias is not None),
            kernel_initializer=tf.keras.initializers.Constant(weight),
            bias_initializer=tf.keras.initializers.Constant(bias) if bias is not None else None
        )
        return layer(input)
    return func

@converter(nn.Linear)
def convert_Linear(self, input, *args, **kwargs):
    weight = self.weight.detach().numpy().T  # Transpose for Keras
    bias = self.bias.detach().numpy() if self.bias is not None else None
    
    def func(input, weight=weight, bias=bias):
        # Create Keras dense layer with pre-set weights
        layer = tf.keras.layers.Dense(
            units=weight.shape[1],
            use_bias=(bias is not None),
            kernel_initializer=tf.keras.initializers.Constant(weight),
            bias_initializer=tf.keras.initializers.Constant(bias) if bias is not None else None
        )
        return layer(input)
    return func

@converter(nn.LayerNorm)
def convert_LayerNorm(self, input, *args, **kwargs):
    weight = self.weight.detach().numpy() if self.weight is not None else None
    bias = self.bias.detach().numpy() if self.bias is not None else None
    eps = self.eps
    
    def func(input, weight=weight, bias=bias, eps=eps):
        # Create Keras layer norm with pre-set weights
        layer = tf.keras.layers.LayerNormalization(
            axis=-1,
            epsilon=eps,
            gamma_initializer=tf.keras.initializers.Constant(weight) if weight is not None else None,
            beta_initializer=tf.keras.initializers.Constant(bias) if bias is not None else None,
            center=(bias is not None),
            scale=(weight is not None)
        )
        return layer(input)
    return func

@converter(nn.Embedding)
def convert_Embedding(self, input, *args, **kwargs):
    weight = self.weight.detach().numpy()
    
    def func(input, weight=weight):
        # Create embedding layer with pre-set weights
        layer = tf.keras.layers.Embedding(
            input_dim=self.num_embeddings,
            output_dim=self.embedding_dim,
            embeddings_initializer=tf.keras.initializers.Constant(weight),
            trainable=False
        )
        return layer(input)
    return func

# FIX: Add converter for tensor shape operations
@converter(torch.Tensor.size, channel_ordering_strategy=ChannelOrderingStrategy.MINIMUM_TRANSPOSITIONS)
def converter_size(input, *args, **kwargs):
    def func(input, *args, **kwargs):
        return input.shape
    return func

# Step 10: Convert to Keras using Nobuco
print("Starting Nobuco conversion with custom converters...")

try:
    keras_model = nobuco.pytorch_to_keras(
        model,
        args=[dummy_pixel_values, dummy_input_ids],
        inputs_channel_order={
            'pixel_values': ChannelOrder.TENSORFLOW,  # Converts to (B, H, W, C)
        },
        outputs_channel_order=ChannelOrder.PYTORCH,
        trace_shape=True,
        verbose=True
    )
    print("✓ Keras conversion successful!")
    
    # Save Keras model for debugging
    keras_model.save("keras_model_debug")
    print("Saved Keras model for inspection")
except Exception as e:
    print(f"Keras conversion failed: {e}")
    print("Attempting fallback with simplified architecture...")
    
    # Fallback: Create a simpler model for conversion
    class SimpleSigLIP(nn.Module):
        def __init__(self, model):
            super().__init__()
            self.model = model
            
        def forward(self, pixel_values, input_ids):
            # Bypass complex operations that might cause issues
            image_embeds, text_embeds = self.model(pixel_values, input_ids)
            return image_embeds, text_embeds
            
    simple_model = SimpleSigLIP(model)
    simple_model.eval()
    
    try:
        keras_model = nobuco.pytorch_to_keras(
            simple_model,
            args=[dummy_pixel_values, dummy_input_ids],
            inputs_channel_order={
                'pixel_values': ChannelOrder.TENSORFLOW,
            },
            outputs_channel_order=ChannelOrder.PYTORCH,
            trace_shape=True,
            verbose=True
        )
        print("✓ Fallback Keras conversion successful!")
    except Exception as e2:
        print(f"Fallback conversion also failed: {e2}")
        print("\nTroubleshooting steps:")
        print("1. Check if the environment variable NOBUCO_DEBUG_WEIGHT_LAYER is set to '1'")
        print("2. Verify all layers have custom converters")
        print("3. Try reducing model complexity")
        print("4. Consider downgrading to TensorFlow 2.15 or upgrading Nobuco")
        print("5. Open an issue on the Nobuco GitHub repository")
        raise

# Step 11: Convert to TFLite
print("Converting to TFLite...")
converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS
]

try:
    tflite_model = converter.convert()
    tflite_path = "siglip-base-patch16-256.tflite"
    with open(tflite_path, "wb") as f:
        f.write(tflite_model)

    print(f"✓ TFLite conversion successful! Model saved to {tflite_path}")
    print(f"Model size: {len(tflite_model)/1024/1024:.2f} MB")
    
    # Step 12: Verify the model
    print("Verifying model...")
    interpreter = tf.lite.Interpreter(model_path=tflite_path)
    interpreter.allocate_tensors()

    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    print("\nModel Inputs:")
    for i, inp in enumerate(input_details):
        print(f"Input {i}: {inp['name']}, Shape: {inp['shape']}, Type: {inp['dtype']}")

    print("\nModel Outputs:")
    for i, out in enumerate(output_details):
        print(f"Output {i}: {out['name']}, Shape: {out['shape']}, Type: {out['dtype']}")

    # Step 13: Test inference
    print("\nTesting inference with dummy inputs...")

    # Prepare inputs in TF format (NHWC for image)
    pixel_values_np = np.random.randn(1, 256, 256, 3).astype(np.float32)  # NHWC format
    input_ids_np = np.ones((1, 64), dtype=np.int64)

    # Set inputs
    interpreter.set_tensor(input_details[0]['index'], pixel_values_np)
    interpreter.set_tensor(input_details[1]['index'], input_ids_np)

    # Run inference
    interpreter.invoke()

    # Get outputs
    image_embeds = interpreter.get_tensor(output_details[0]['index'])
    text_embeds = interpreter.get_tensor(output_details[1]['index'])

    print("Inference successful!")
    print(f"Image embeds shape: {image_embeds.shape}")
    print(f"Text embeds shape: {text_embeds.shape}")
    print(f"Sample image embeds: {image_embeds[0, :5]}")
    print(f"Sample text embeds: {text_embeds[0, :5]}")

    print("\n🎉 Conversion and verification complete!")

except Exception as e:
    print(f"TFLite conversion failed: {e}")
    print("\nTroubleshooting steps:")
    print("1. Try converting the saved Keras model separately:")
    print("   converter = tf.lite.TFLiteConverter.from_saved_model('keras_model_debug')")
    print("2. Try adding these optimizations:")
    print("   converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]")
    print("   converter.target_spec.supported_types = [tf.float16]")
    print("3. Reduce model complexity by removing specific layers")
    print("4. If using GPU, add: converter.experimental_new_converter = True")
